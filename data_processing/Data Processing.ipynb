{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1699666000990,
     "user": {
      "displayName": "Rico Rodriguez Passanisi",
      "userId": "16892258032340242684"
     },
     "user_tz": 480
    },
    "id": "3ZXzmquTPYnk"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy\n",
    "import multiprocessing as mp\n",
    "import tqdm\n",
    "\n",
    "# from google.colab import drive\n",
    "import os\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utiwCYAlP7xC"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Compressed Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 13931,
     "status": "ok",
     "timestamp": 1699666160447,
     "user": {
      "displayName": "Rico Rodriguez Passanisi",
      "userId": "16892258032340242684"
     },
     "user_tz": 480
    },
    "id": "eVg2cKFbeyAF"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./weatherdata.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPW4zKT76IE4"
   },
   "source": [
    "### Load Data (Generating Job Queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CHECK THIS BEFORE RUNNING DATA COLLECTION\n",
    "\n",
    "interval = 5 was used for data collection for training final IAE model\n",
    "\"\"\"\n",
    "\n",
    "interval = 20 # Step size in degrees lat/long when generating the job queue (Higher = less computation time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./weatherdata/\"\n",
    "random.seed(0)\n",
    "\n",
    "files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('04I.txt.gz')]\n",
    "sorted_file_list = sorted(files)\n",
    "\n",
    "\n",
    "\n",
    "# Generate job queue for parallelization\n",
    "# Each job is responsible for a 5x5 lat/long square on the globe\n",
    "def gen_job_queue(sorted_file_list):\n",
    "    job_buckets = []\n",
    "    job_num = 0\n",
    "    interval = 20\n",
    "    box_size = 5\n",
    "    bucket_index = 0\n",
    "    for min_lat in np.arange(-59.75, 54.95, interval): #90, 20)):\n",
    "        max_lat = min_lat + box_size\n",
    "        if max_lat > 90:\n",
    "            continue\n",
    "            \n",
    "        for min_lon in np.arange(-179.95, 179.95, interval): #180, 20)):\n",
    "            max_lon = min_lon + box_size\n",
    "            if max_lon > 180:\n",
    "                continue\n",
    "                \n",
    "            job_list = []\n",
    "            for i,file_name in enumerate(sorted_file_list):\n",
    "                job_list.append((np.round(min_lat,2), np.round(max_lat,2), np.round(min_lon,2), np.round(max_lon,2), file_name, i, job_num, bucket_index))\n",
    "                job_num += 1\n",
    "                \n",
    "            job_buckets.append(job_list)\n",
    "            bucket_index += 1\n",
    "            \n",
    "    return job_buckets\n",
    "\n",
    "jobQ = gen_job_queue(sorted_file_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input how many threads you want to process the data at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 31 # IF THIS NUMBER IS CLOSE TO YOUR LOGICAL PROCESSOR COUNT THEN EXPECT EXTREME SLOWDOWN ON YOUR COMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "error",
     "timestamp": 1699666781422,
     "user": {
      "displayName": "Rico Rodriguez Passanisi",
      "userId": "16892258032340242684"
     },
     "user_tz": 480
    },
    "id": "LKyzSYF-eyCz",
    "outputId": "ee7b81d7-3e6e-4f85-a6f8-f61cc080d918"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [06:54<00:00,  3.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from job_functions import parse_file, make_trios\n",
    "\n",
    "def collect_all_data(jobQ):\n",
    "    trios = []\n",
    "    for bucket in tqdm.tqdm(jobQ):\n",
    "        L = []\n",
    "        with mp.Pool(NUM_THREADS) as pool:\n",
    "            L = pool.map(parse_file, bucket)\n",
    "        new_trios = make_trios(L)\n",
    "\n",
    "        # Filter out jobs that were excluded because there wasn't enough data for that square\n",
    "        new_trios = list(filter(lambda item: len(item.shape) != 1, new_trios))\n",
    "        trios.extend(new_trios)\n",
    "    return trios\n",
    "\n",
    "        \n",
    "DATA = collect_all_data(jobQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to numpy file for use in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TClW8gf93kF3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ricof\\Documents\\ECS-289G\\repo\\data_processing\\Data Processing.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ricof/Documents/ECS-289G/repo/data_processing/Data%20Processing.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata_more.npy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ricof/Documents/ECS-289G/repo/data_processing/Data%20Processing.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     np\u001b[39m.\u001b[39msave(f, np\u001b[39m.\u001b[39marray(DATA))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "with open('data.npy', 'wb') as f:\n",
    "    np.save(f, np.array(DATA))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
